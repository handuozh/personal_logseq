#+TITLE: Non-local

** Attention
:PROPERTIES:
:heading: true
:END:
*** [[Attention is all you need]]
*** Attention Mechanism可以帮助模型对输入的X每个部分赋予不同的权重，抽取出更加关键及重要的信息，使模型做出更加准确的判断，同时不会对模型的计算和存储带来更大的开销，这也是Attention Mechanism应用如此广泛的原因
*** 注意力机制可分为两种
**** 一种是软注意力(soft attention)
***** 软注意力更关注**区域或者通道**，而且软注意力是确定性的注意力
***** 学习完成后直接可以通过网络生成，最关键的地方是软注意力是**可微的differentiable**，这是一个非常重要的地方。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重
***** 在cv中，很多领域的相关工作(例如，分类、检测、分割、生成模型、视频处理等)都在使用Soft Attention，典型代表：SENet、SKNet
**** 一种则是强注意力(hard attention)
***** 强注意力是更加关注点，也就是图像中的每个点都有可能延伸出注意力，同时强注意力是一个随机的预测过程，更强调动态变化
***** 当然，最关键是强注意力是一个**不可微**的注意力，训练过程往往是通过增强学习([[reinforcement learning]]) 来完成的
*** CV与attention结合
**** 深度学习与视觉注意力机制结合的研究工作，大多数是集中于使用掩码(mask)来形成注意力机制
***** Mask的原理在于通过另一层新的权重，将图片数据中关键的特征标识出来，通过学习训练，让深度神经网络学到每一张新图片中需要关注的区域，也就形成了注意力
*****
** 长距离关系建模
:PROPERTIES:
:heading: true
:END:
*** 为了解决卷积神经网络的有效感受野受限的问题
*** 分为两种
**** local-based 增大感受野
***** [[dilated conv]], [[Deformable Convolution Network]] , [[aspp]]
**** global-based 采用 [[attention]] 机制
***** 通过建模二元关系获得全局感受野
***** 但是建模一元或二元关系的方法，无法感知其他物体的影响
****** 例如，当两个同类物体被背景隔离，这对于instance相关的任务而言，期望两者具有较低的相关性
****** 但是在没有position encoding的前提下，这些方法会输出较高的相关性
****** 反映在可视化上，即^^很难保留物体的细节或者结构化信息^^
:PROPERTIES:
:id: 5fe93262-f9e8-408a-a255-846024a37549
:END:
**** [[https://i.imgur.com/tfEfAsk.png][long-range]]
** 为了解决global-based的相关性问题
*** [[Learnable Tree Filter]]
** Attention
***
$$y_i=\frac{1}{C(x)}\sum\limits_{\forall j}f(x_i,x_j)g(x_j)$$
**** $x$ input feauture map
**** $f(\cdot)$ the similarity between $x_i$ and $x_j$
**** $g(\cdot)$ the representation of feauture map
**** $y$ the final output after normalization of response factor $C(x)$
*** $i$ 当前位置的响应, $j$ 全局响应
***
