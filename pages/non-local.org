#+TITLE: Non-local

** Attention
:PROPERTIES:
:heading: true
:END:
*** [[Attention is all you need]]
*** Attention Mechanism可以帮助模型对输入的X每个部分赋予不同的权重，抽取出更加关键及重要的信息，使模型做出更加准确的判断，同时不会对模型的计算和存储带来更大的开销，这也是Attention Mechanism应用如此广泛的原因
*** 注意力机制可分为两种
**** 一种是软注意力(soft attention)
***** 软注意力更关注区域或者通道，而且软注意力是确定性的注意力
***** 学习完成后直接可以通过网络生成，最关键的地方是软注意力是可微的，这是一个非常重要的地方。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重
***** 在cv中，很多领域的相关工作(例如，分类、检测、分割、生成模型、视频处理等)都在使用Soft Attention，典型代表：SENet、SKNet
**** 一种则是强注意力(hard attention)
*****
***
** 长距离关系建模
:PROPERTIES:
:heading: true
:END:
*** 为了解决卷积神经网络的有效感受野受限的问题
*** 分为两种
**** local-based 增大感受野
***** [[dilated conv]], [[Deformable Convolution Network(DCN)]] , [[aspp]]
**** global-based 采用 [[attention]] 机制
***** 通过建模二元关系获得全局感受野
***** 但是建模一元或二元关系的方法，无法感知其他物体的影响
****** 例如，当两个同类物体被背景隔离，这对于instance相关的任务而言，期望两者具有较低的相关性
****** 但是在没有position encoding的前提下，这些方法会输出较高的相关性
****** 反映在可视化上，即^^很难保留物体的细节或者结构化信息^^
**** [[https://i.imgur.com/tfEfAsk.png][long-range]]
** 为了解决global-based的相关性问题
*** [[Learnable Tree Filter]]
** Attention
***
$$y_i=\frac{1}{C(x)}\sum\limits_{\forall j}f(x_i,x_j)g(x_j)$$
**** $x$ input feauture map
**** $f(\cdot)$ the similarity between $x_i$ and $x_j$
**** $g(\cdot)$ the representation of feauture map
**** $y$ the final output after normalization of response factor $C(x)$
*** $i$ 当前位置的响应, $j$ 全局响应
***
