---
title: self-attention
---

## #seq2seq #attention 

## The original self-attention paper uses [[LSTM]]
### Long Short-Term Memory-Networks for Machine Reading, EMNLP, 2016.
## Tutorial on youtube by Wang Shusen
##
