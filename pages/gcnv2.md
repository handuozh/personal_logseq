---
title: GCNv2
public: true
published: true
permalink: gcnv2
---

## DONE Note not finished!
:PROPERTIES:
:done: 1606375371048
:later: 1606375366177
:END:
## Meta Data
:PROPERTIES:
:heading: true
:END:
### #title GCNv2: Efficient Correspondence Prediction for Real-Time SLAM, 2019
### #topic #Keypoint [[Detect-then-describe]] #Efficient #[[Binary Descriptors]]

## Abstract
:PROPERTIES:
:heading: true
:END:
### GCNv2 predicts both keypoints (probability map) and descriptors (dense feature map $\mathbf{f}$).
### Inspired by GCN and [[Superpoint]] [[Superpoint]]
#### Perform predictions at low resolution

#### Then [[pixel-shuffle]] the 256 channels probability map to the original resolution

#### Finally [[NMS]] over full resolution map to bilinearly sample the corresponding feature vectors from the low resolution dense feature map.

### 

## 1. Network Structure
:PROPERTIES:
:heading: true
:END:
### ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FSLAM%2FeWPtCLCfIH.png?alt=media&token=c85b5710-08e4-45e8-b70f-8a18bfd89ec1)

### The orange blocks are conv layers with [[ELU]] non-linearity after each set of blocks

### $$I$$ divided by some number is the resolution of the current feature map

## 2. Feature Extractor
:PROPERTIES:
:heading: true
:END:
### Notation
#### Dense feature map $$\mathbf{f}$$ at low resolution

#### [[pixel-shuffle]] keypoint probability map $$\mathbf{o}$$ at original resolution
### Binarized Descriptor
#### To accelerate, and to match [[ORB]]: feature vector $\mathbf{b}$ set to 256
#### Add a hard sign function non-linearity as a binary activation layer on top of final output

#### Make it ^^differentiable^^
##### Straight-through estimator:: Indicator of the hard sign function for backpropagating the gradients. It approximates binarization during backpropagation and avoids over-penalization to feature responses already passed the decision boundary by 1.

##### 

### Nested [[metric-learning]]
#### Pixel-wise [[metric-learning]] is used in a [[nearest-neighbour]] manner.

#### [[Triplet loss]] for binarized feature
##### (2)   $$L_{feat}=\sum\limits_i \max\left(0,d(\mathbf{x}_i^{cur},\mathbf{x}_{i,+}^{tar})-d(\mathbf{x}_i^{cur},\mathbf{x}_{i,-}^{tar})+m\right)$$
        $$d(\mathbf{x}^{cur},\mathbf{x}^{tar})=||\mathbf{b}^{cur}(\mathbf{x}^{cur})-\mathbf{b}^{tar}(\mathbf{x}^{tar})||_2$$
where $m$ is the distance margin for truncation, $d(\cdot,\cdot)$ is the squared [[Hamming Distance]] for binarized features, which is faster and better convergence.
##### $$(\mathbf{x}_i^{cur},\mathbf{x}_{i,+}^{tar})$$ is a matching pair with GT camera poses from the training data:
###### $$\mathbf{x}_{i,+}^{tar}=\mathbf{\pi}^{-1}\left(\mathbf{R}_{gt}\mathbf{\pi}(\mathbf{x}_i^{cur},z_i)+\mathbf{t}_{gt}\right)$$
where $$\mathbf{\pi}$$ unproject a pixel from image plane to 3D place using the given 2D coordinates $$\mathbf{x}_i^{cur}$$ and depth $$z_i$$.

##### $$(\mathbf{x}_i^{cur},\mathbf{x}_{i,-}^{tar})$$ is a non-matching pair retrieved by exhaustive [[Negative Sample]] mining described below:
###### ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FSLAM%2FBXSudpZ01W.png?alt=media&token=383880ad-c38d-4f93-97c3-3cce06fc9e7e)

###### The exhaustive search will further penalize the already matched features with [[relaxed criteria]]. 
####### Relaxed criteria:: Increase the tolerance to potentially noisy data.

## 3. Distributed Keypoint Detector
### Treat keypoint detection as a binary classification problem.

### The target of the probability map $$o$$ is a mask with 1 and 0

### Weighted [[cross-entropy]] is used as objective function for the training.
#### Loss is always evaluated on 2 consecutive frames to enhance keypoint consistency.

#### (4)    $$L_{det}=L_{ce}\left(\mathbf{o}^{cur}(\mathbf{x}^{cur})\right) + L_{ce}\left(\mathbf{o}^{tar}(\mathbf{x}^{tar}_{+})\right)$$
         $$L_{ce}(\mathbf{o(x)})=-\alpha_1 \sum\limits_i c_{\mathbf{x}_i}log{(\mathbf{o(x)}_i)}-\alpha_2 \sum\limits_i (1-c_{\mathbf{x}_i})log{(1-\mathbf{o(x)}_i)}$$
         where $$\alpha_i$$ and $$\alpha_2$$ handle unbalanced classes to prevent pixels (not keypoints) from dominating the loss.

### GT is generated by [[Shi-Tomasi]] corners ina $$16\times 16$$ grid and warp them to the next frame using eq. (3)