[
  {"id":"bergmannTrackingBellsWhistles2019","abstract":"The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.","accessed":{"date-parts":[[2021,1,4]]},"author":[{"family":"Bergmann","given":"Philipp"},{"family":"Meinhardt","given":"Tim"},{"family":"Leal-Taixe","given":"Laura"}],"container-title":"arXiv:1903.05625 [cs]","issued":{"date-parts":[[2019,8,17]]},"source":"arXiv.org","title":"Tracking without bells and whistles","type":"article-journal","URL":"http://arxiv.org/abs/1903.05625"},
  {"id":"gaoGraphConvolutionalTracking2019","abstract":"Tracking by siamese networks has achieved favorable performance in recent years. However, most of existing siamese methods do not take full advantage of spatialtemporal target appearance modeling under different contextual situations. In fact, the spatial-temporal information can provide diverse features to enhance the target representation, and the context information is important for online adaption of target localization. To comprehensively leverage the spatial-temporal structure of historical target exemplars and get beneﬁt from the context information, in this work, we present a novel Graph Convolutional Tracking (GCT) method for high-performance visual tracking. Speciﬁcally, the GCT jointly incorporates two types of Graph Convolutional Networks (GCNs) into a siamese framework for target appearance modeling. Here, we adopt a spatial-temporal GCN to model the structured representation of historical target exemplars. Furthermore, a context GCN is designed to utilize the context of the current frame to learn adaptive features for target localization. Extensive results on 4 challenging benchmarks show that our GCT method performs favorably against state-of-the-art trackers while running around 50 frames per second.","accessed":{"date-parts":[[2020,12,22]]},"author":[{"family":"Gao","given":"Junyu"},{"family":"Zhang","given":"Tianzhu"},{"family":"Xu","given":"Changsheng"}],"container-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2019.00478","event":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","event-place":"Long Beach, CA, USA","ISBN":"978-1-72813-293-8","issued":{"date-parts":[[2019,6]]},"language":"en","page":"4644-4654","publisher":"IEEE","publisher-place":"Long Beach, CA, USA","source":"DOI.org (Crossref)","title":"Graph Convolutional Tracking","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8953448/"},
  {"id":"linVideoInstanceSegmentation","abstract":"We propose a modiﬁed variational autoencoder (VAE) architecture built on top of Mask R-CNN for instance-level video segmentation and tracking. The method builds a shared encoder and three parallel decoders, yielding three disjoint branches for predictions of future frames, object detection boxes, and instance segmentation masks. To effectively solve multiple learning tasks, we introduce a Gaussian Process model to enhance the statistical representation of VAE by relaxing the prior strong independent and identically distributed (iid) assumption of conventional VAEs and allowing potential correlations among extracted latent variables. The network learns embedded spatial interdependence and motion continuity in video data and creates a representation that is effective to produce high-quality segmentation masks and track multiple instances in diverse and unstructured videos. Evaluation on a variety of recently introduced datasets shows that our model outperforms previous methods and achieves the new best in class performance.","author":[{"family":"Lin","given":"Chung-Ching"},{"family":"Hung","given":"Ying"},{"family":"Feris","given":"Rogerio"},{"family":"He","given":"Linglin"}],"language":"en","page":"11","source":"Zotero","title":"Video Instance Segmentation Tracking With a Modified VAE Architecture","type":"article-journal"},
  {"id":"songOnlineMultiObjectTracking2020","abstract":"In this paper, we propose a highly practical fully online multi-object tracking and segmentation (MOTS) method that uses instance segmentation results as an input in video. The proposed method exploits the Gaussian mixture probability hypothesis density (GMPHD) filter for online approach which is extended with a hierarchical data association (HDA) and a simple affinity fusion (SAF) model. HDA consists of segment-to-track and track-to-track associations. To build the SAF model, an affinity is computed by using the GMPHD filter that is represented by the Gaussian mixture models with position and motion mean vectors, and another affinity for appearance is computed by using the responses from single object tracker such as the kernalized correlation filters. These two affinities are simply fused by using a score-level fusion method such as Min-max normalization. In addition, to reduce false positive segments, we adopt Mask IoU based merging. In experiments, those key modules, i.e., HDA, SAF, and Mask merging show incremental improvements. For instance, ID-switch decreases by half compared to baseline method. In conclusion, our tracker achieves state-of-the-art level MOTS performance.","accessed":{"date-parts":[[2020,12,17]]},"author":[{"family":"Song","given":"Young-min"},{"family":"Jeon","given":"Moongu"}],"container-title":"arXiv:2009.00100 [cs]","issued":{"date-parts":[[2020,8,31]]},"source":"arXiv.org","title":"Online Multi-Object Tracking and Segmentation with GMPHD Filter and Simple Affinity Fusion","type":"article-journal","URL":"http://arxiv.org/abs/2009.00100"},
  {"id":"voigtlaenderMOTSMultiObjectTracking2019","abstract":"This paper extends the popular task of multi-object tracking to multi-object tracking and segmentation (MOTS). Towards this goal, we create dense pixel-level annotations for two existing tracking datasets using a semi-automatic annotation procedure. Our new annotations comprise 65,213 pixel masks for 977 distinct objects (cars and pedestrians) in 10,870 video frames. For evaluation, we extend existing multi-object tracking metrics to this new task. Moreover, we propose a new baseline method which jointly addresses detection, tracking, and segmentation with a single convolutional network. We demonstrate the value of our datasets by achieving improvements in performance when training on MOTS annotations. We believe that our datasets, metrics and baseline will become a valuable resource towards developing multi-object tracking approaches that go beyond 2D bounding boxes. We make our annotations, code, and models available at https: //www.vision.rwth-aachen.de/page/mots.","accessed":{"date-parts":[[2020,12,17]]},"author":[{"family":"Voigtlaender","given":"Paul"},{"family":"Krause","given":"Michael"},{"family":"Osep","given":"Aljosa"},{"family":"Luiten","given":"Jonathon"},{"family":"Sekar","given":"Berin Balachandar Gnana"},{"family":"Geiger","given":"Andreas"},{"family":"Leibe","given":"Bastian"}],"container-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2019.00813","event":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","event-place":"Long Beach, CA, USA","ISBN":"978-1-72813-293-8","issued":{"date-parts":[[2019,6]]},"language":"en","page":"7934-7943","publisher":"IEEE","publisher-place":"Long Beach, CA, USA","source":"DOI.org (Crossref)","title":"MOTS: Multi-Object Tracking and Segmentation","title-short":"MOTS","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8953401/"},
  {"id":"weng3DMultiObjectTracking","author":[{"family":"Weng","given":"Xinshuo"},{"family":"Wang","given":"Jianren"},{"family":"Held","given":"David"},{"family":"Kitani","given":"Kris"}],"language":"en","page":"8","source":"Zotero","title":"3D Multi-Object Tracking: A Baseline and New Evaluation Metrics","type":"article-journal"},
  {"id":"wengParallelized3DTracking","abstract":"Multi-object tracking (MOT) and trajectory forecasting are two critical components in modern 3D perception systems that require accurate modeling of multi-agent interaction. We hypothesize that it is beneﬁcial to unify both tasks under one framework in order to learn a shared feature representation of agent interaction. Furthermore, instead of performing tracking and forecasting sequentially which can propagate errors from tracking to forecasting, we propose a parallelized framework to mitigate the issue. Also, our proposed parallel track-forecast framework incorporates two additional novel computational units. First, we employ a feature interaction technique by introducing Graph Neural Networks (GNNs) to capture the way in which agents interact with one another. The GNN is able to improve discriminative feature learning for MOT association and provide socially-aware contexts for trajectory forecasting. Second, we use a diversity sampling function to improve the quality and diversity of our forecasted trajectories. The learned sampling function is trained to efﬁciently extract a variety of outcomes from a generative trajectory distribution and helps avoid the problem of generating duplicate trajectory samples. We evaluate on KITTI and nuScenes datasets showing that our method with socially-aware feature learning and diversity sampling achieves new state-of-the-art performance on both 3D MOT and trajectory forecasting.","author":[{"family":"Weng","given":"Xinshuo"},{"family":"Yuan","given":"Ye"},{"family":"Kitani","given":"Kris"}],"language":"en","page":"9","source":"Zotero","title":"Parallelized 3D Tracking and Forecasting with Graph Neural Networks and Diversity Sampling","type":"article-journal"}
]
